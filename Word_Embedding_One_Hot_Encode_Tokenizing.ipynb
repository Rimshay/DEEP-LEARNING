{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Embedding/One Hot Encode/Tokenizing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNG1daYIwBe9qJhZ2/GkXgg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rimshay/DEEP-LEARNING/blob/main/Word_Embedding_One_Hot_Encode_Tokenizing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.1 Word-level one-hot encoding (toy example)**"
      ],
      "metadata": {
        "id": "jW_yusN8ymZN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "86fMQN_Jyhcm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.', \"پاکستان زندہ باد\"]\n",
        "token_index = {}\n",
        "for sample in samples:\n",
        "    for word in sample.split():\n",
        "        if word not in token_index:\n",
        "            token_index[word] = len(token_index) + 1\n",
        "            \n",
        "max_length = 10\n",
        "results = np.zeros(shape=(len(samples),\n",
        "                          max_length,\n",
        "                          max(token_index.values()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = token_index.get(word)\n",
        "        results[i, j, index] = 1. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_index"
      ],
      "metadata": {
        "id": "HuWtN7CWyqqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "00YRu6U0yswU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results[2]"
      ],
      "metadata": {
        "id": "96lNnOqPyvXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.2 Character-level one-hot encoding (toy example)**"
      ],
      "metadata": {
        "id": "7nI2QXsGyxLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "characters = string.printable\n",
        "token_index = dict(zip(range(1, len(characters) + 1), characters))\n",
        "max_length = 50\n",
        "results = np.zeros((len(samples), max_length, max(token_index.keys()) + 1))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, character in enumerate(sample):\n",
        "        index = token_index.get(character)\n",
        "        results[i, j, index] = 1."
      ],
      "metadata": {
        "id": "8VnUe_mtyzlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(token_index))\n",
        "token_index"
      ],
      "metadata": {
        "id": "Ww9mE-aryvVq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "nyKICCrZy4uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(results[0]))\n",
        "results[0]"
      ],
      "metadata": {
        "id": "9JoUPnkwy7AR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.3 Using Keras for word-level one-hot encoding**"
      ],
      "metadata": {
        "id": "zwR1LMj7y9qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(samples)\n",
        "sequences = tokenizer.texts_to_sequences(samples)\n",
        "one_hot_results = tokenizer.texts_to_matrix(samples, mode='binary')\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "metadata": {
        "id": "ONgGx0e2y_Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.texts_to_matrix(samples)"
      ],
      "metadata": {
        "id": "xxnVapQ6zCx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer"
      ],
      "metadata": {
        "id": "CQmoNUj1zEta"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences"
      ],
      "metadata": {
        "id": "vY3HtjGvzGKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_results"
      ],
      "metadata": {
        "id": "3ksp4eVczIi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_hot_results.shape"
      ],
      "metadata": {
        "id": "cggQ7ZZ5zKTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index "
      ],
      "metadata": {
        "id": "MMREC0qozMLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.4 Word-level one-hot encoding with hashing trick (toy example)**"
      ],
      "metadata": {
        "id": "Kr2u5trezPxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "samples = ['The cat sat on the mat.', 'The dog ate my homework.']\n",
        "dimensionality = 1000\n",
        "max_length = 10\n",
        "results = np.zeros((len(samples), max_length, dimensionality))\n",
        "for i, sample in enumerate(samples):\n",
        "    for j, word in list(enumerate(sample.split()))[:max_length]:\n",
        "        index = abs(hash(word)) % dimensionality\n",
        "        results[i, j, index] = 1."
      ],
      "metadata": {
        "id": "A9f26bc9zRCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results.shape"
      ],
      "metadata": {
        "id": "--nO0D0WzU-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abs(hash('abx'))"
      ],
      "metadata": {
        "id": "hV1zn6wXzXEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abs(hash('the')) % max_length"
      ],
      "metadata": {
        "id": "SeewgynfzYtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.5 Instantiating an Embedding layer**"
      ],
      "metadata": {
        "id": "Yy_wGnP6zbMH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Embedding\n",
        "embedding_layer = Embedding(1000, 64)\n",
        "embedding_layer"
      ],
      "metadata": {
        "id": "GuqKjB2Zzfhu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.6 Loading the IMDB data for use with an Embedding layer**"
      ],
      "metadata": {
        "id": "-W2IM7QMzhj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras import preprocessing\n",
        "max_features = 10000\n",
        "maxlen = 20\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "\n",
        "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
      ],
      "metadata": {
        "id": "9Zm9l6Aczi4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.7 Using an Embedding layer and classifier on the IMDB data**"
      ],
      "metadata": {
        "id": "jKYQo-NSzn5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(10000, x`8, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
        "model.summary()\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "id": "idWp950izneW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.8 Processing the labels of the raw IMDB data**"
      ],
      "metadata": {
        "id": "KQ1EUUsIzvDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "imdb_dir = 'aclImdb/'\n",
        "train_dir = os.path.join(imdb_dir, 'train')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(train_dir, label_type)\n",
        "    for fname in os.listdir(dir_name):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding='utf-8')\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)"
      ],
      "metadata": {
        "id": "PzweQp-Azuog"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.9 Tokenizing the text of the raw IMDB data**"
      ],
      "metadata": {
        "id": "SSyvfrk3zz8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "maxlen = 100\n",
        "training_samples = 200\n",
        "validation_samples = 10000\n",
        "max_words = 10000\n",
        "tokenizer = Tokenizer(num_words=max_words)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))\n",
        "data = pad_sequences(sequences, maxlen=maxlen)\n",
        "labels = np.asarray(labels)\n",
        "print('Shape of data tensor:', data.shape)\n",
        "print('Shape of label tensor:', labels.shape)\n",
        "\n",
        "indices = np.arange(data.shape[0])\n",
        "np.random.shuffle(indices)\n",
        "data = data[indices]\n",
        "labels = labels[indices]\n",
        "x_train = data[:training_samples]\n",
        "y_train = labels[:training_samples]\n",
        "x_val = data[training_samples: training_samples + validation_samples]\n",
        "y_val = labels[training_samples: training_samples + validation_samples]"
      ],
      "metadata": {
        "id": "k-8pgjSMz28o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.10 Parsing the GloVe word-embeddings file**"
      ],
      "metadata": {
        "id": "Qvdoeh1iz5hy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#{The :[00000100001],\n",
        "#\"ABC\": None}\n",
        "\n",
        "glove_dir = 'glove.6B'\n",
        "embeddings_index = {}\n",
        "f = open(os.path.join(glove_dir, 'glove.6B.100d.txt'), encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "metadata": {
        "id": "LbaxuaEbz79a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.11 Preparing the GloVe word-embeddings matrix**"
      ],
      "metadata": {
        "id": "MGukq8fdz-6Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 100\n",
        "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_words:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "        if embedding_vector is not None:\n",
        "            embedding_matrix[i] = embedding_vector"
      ],
      "metadata": {
        "id": "r9dh3esq0Bbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.12 Model definition**"
      ],
      "metadata": {
        "id": "NEQ8U8re0DHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "diiCmU-L0Goo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.13 Loading pretrained word embeddings into the Embedding layer**"
      ],
      "metadata": {
        "id": "RRGXpPYO0JJ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False"
      ],
      "metadata": {
        "id": "35b0NUUO0LRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.14 Training and evaluation**"
      ],
      "metadata": {
        "id": "XYPpw8HY0NGX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "validation_data=(x_val, y_val))\n",
        "model.save_weights('pre_trained_glove_model.h5')"
      ],
      "metadata": {
        "id": "6o8R3yZu0QMP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.15 Plotting the results**"
      ],
      "metadata": {
        "id": "GVXWJqN30Svf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ckoNK_On0Vtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.16 Training the same model without pretrained word embeddings**"
      ],
      "metadata": {
        "id": "n0_Vl6gJ0ZFe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "j7fOn3Px0ceR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['acc'])\n",
        "history = model.fit(x_train, y_train,\n",
        "                    epochs=10,\n",
        "                    batch_size=32,\n",
        "validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "4TImMIDo0f6G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.17 Tokenizing the data of the test set**"
      ],
      "metadata": {
        "id": "9rFeepLc0iy-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dir = os.path.join(imdb_dir, 'test')\n",
        "labels = []\n",
        "texts = []\n",
        "for label_type in ['neg', 'pos']:\n",
        "    dir_name = os.path.join(test_dir, label_type)\n",
        "    for fname in sorted(os.listdir(dir_name)):\n",
        "        if fname[-4:] == '.txt':\n",
        "            f = open(os.path.join(dir_name, fname), encoding='utf-8')\n",
        "            texts.append(f.read())\n",
        "            f.close()\n",
        "            if label_type == 'neg':\n",
        "                labels.append(0)\n",
        "            else:\n",
        "                labels.append(1)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "x_test = pad_sequences(sequences, maxlen=maxlen)\n",
        "y_test = np.asarray(labels)"
      ],
      "metadata": {
        "id": "a-sXq1ac0lAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listing 6.18 Evaluating the model on the test set**"
      ],
      "metadata": {
        "id": "ynth_ie_0n_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights('pre_trained_glove_model.h5')\n",
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "ZMib_J5g0qhg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}